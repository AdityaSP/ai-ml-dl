{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h4>Classification Overview</h4>\n",
    "<ul>\n",
    "<li>Predict a binary class as output based on given features.\n",
    "</li>\n",
    "\n",
    "<li>Examples: Do we need to follow up on a customer review? Is this transaction fraudulent or valid one? Are there signs of onset of a medical condition or disease? Is this considered junk food or not?</li>\n",
    "\n",
    "<li>Linear Model. Estimated Target = w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> \n",
    "+ w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> \n",
    "+ … + w<sub>n</sub>x<sub>n</sub><br>\n",
    "where, w is the weight and x is the feature\n",
    "</li>\n",
    "\n",
    "<li><b>Logistic Regression</b>. Estimated Probability = <b>sigmoid</b>(w<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> \n",
    "+ w<sub>2</sub>x<sub>2</sub> + w<sub>3</sub>x<sub>3</sub> \n",
    "+ … + w<sub>n</sub>x<sub>n</sub>)<br>\n",
    "where, w is the weight and x is the feature\n",
    "</li>\n",
    "<li>Linear model output is fed thru a sigmoid or logistic function to produce the probability.</li>\n",
    "<li>Predicted Value: Probability of a binary outcome.  Closer to 1 is positive class, closer to 0 is negative class</li>\n",
    "<li>Algorithm Used: Logistic Regression. Objective is to find the weights w that maximizes separation between the two classes</li>\n",
    "<li>Optimization: Stochastic Gradient Descent. Seeks to minimize loss/cost so that predicted value is as close to actual as possible</li>\n",
    "<li>Cost/Loss Calculation: Logistic loss function</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid or logistic function\n",
    "# For any x, output is bounded to 0 & 1.\n",
    "def sigmoid_func(x):\n",
    "    return 1.0/(1+math.exp(-x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_func(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_func(-100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_func(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Sigmoid function example\n",
    "x = pd.Series(np.arange(-8,8,0.5))\n",
    "y = x.map(sigmoid_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(x,y)\n",
    "plt.ylim((-0.2,1.2))\n",
    "plt.xlabel('input')\n",
    "plt.ylabel('sigmoid output')\n",
    "plt.grid(True)\n",
    "\n",
    "plt.axvline(x=0,ymin=0,ymax=1, ls='dashed')\n",
    "plt.axhline(y=0.5,xmin=0,xmax=10, ls='dashed')\n",
    "plt.axhline(y=1.0,xmin=0,xmax=10,color='r')\n",
    "plt.axhline(y=0.0,xmin=0,xmax=10,color='r')\n",
    "plt.title('Sigmoid')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example Dataset - Hours spent and Exam Results: \n",
    "https://en.wikipedia.org/wiki/Logistic_regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sigmoid function produces an output between 0 and 1 no.  Input closer to 0 produces and output of 0.5 probability.  Negative input produces value less than 0.5 while positive input produces value greater than 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = \\\n",
    "r'HoursExamResult.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Input Feature: Hours<br>\n",
    "Output: Pass (1 = pass, 0 = fail)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# optimal weights given in the wiki dataset\n",
    "def straight_line(x):\n",
    "    return 1.5046*x - 4.0777 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_linear = df.Hours.map(straight_line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_linear"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.Pass==1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.scatter(x=df.Hours,y=y_linear,color='b',label='linear')\n",
    "plt.scatter(x=df[df.Pass==1].Hours,y=df[df.Pass==1].Pass, color='g', label='pass')\n",
    "plt.scatter(x=df[df.Pass==0].Hours,y=df[df.Pass==0].Pass, color='r',label='fail')\n",
    "plt.xlim((0,7))\n",
    "plt.ylim((-1,2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Generate probability by running feature thru the linear model and then thru sigmoid function\n",
    "y_vals = df.Hours.map(straight_line).map(sigmoid_func)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(x=df.Hours,y=y_vals,color='b',label='logistic')\n",
    "plt.scatter(x=df[df.Pass==1].Hours,y=df[df.Pass==1].Pass, color='g', label='pass')\n",
    "plt.scatter(x=df[df.Pass==0].Hours,y=df[df.Pass==0].Pass, color='r',label='fail')\n",
    "plt.title('Hours Spent Reading - Pass Probability')\n",
    "plt.xlabel('Hours')\n",
    "plt.ylabel('Pass Probability')\n",
    "#plt.grid(True)\n",
    "#plt.legend()\n",
    "plt.xlim((0,7))\n",
    "plt.ylim((-0.2,1.5))\n",
    "\n",
    "\n",
    "plt.axvline(x=3,ymin=0,ymax=1)\n",
    "plt.axhline(y=0.6,xmin=0,xmax=6, label='cutoff at 0.6', ls='dashed')\n",
    "#plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "myhours = 10\n",
    "y = straight_line(myhours)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sigmoid_func(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At 2.7 hours of study time, we hit 0.5 probability.  So, any student who spent 2.7 hours or more would have a higher probability of passing the exam.\n",
    "\n",
    "In the above example,<br>\n",
    "1. Top right quadrant = true positive. pass got classified correctly as pass\n",
    "2. Bottom left quadrant = true negative. fail got classified correctly as fail\n",
    "3. Top left quadrant = false negative. pass got classified as fail\n",
    "4. Bottom right quadrant = false positive. fail got classified as pass\n",
    "\n",
    "Cutoff can be adjusted; instead of 0.5, cutoff could be established at 0.4 or 0.6 depending on the nature of problem and impact of misclassification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<h4>Summary</h4>\n",
    "<p><b>Binary Classifier</b> Predicts positive class probability of an observation </p>\n",
    "<p><b>Logistic or Sigmod function</b> has an important property where output is between 0 and 1 for any input.  This output is used by binary classifiers as a probability of positive class</p>\n",
    "\n",
    "<p><b>True Positive</b> - Samples that are actual-positives correctly predicted as positive </p>\n",
    "<p><b>True Negative</b> - Samples that are actual-negatives correctly predicted as negative </p>\n",
    "<p><b>False Negative</b> - Samples that are actual-positives incorrectly predicted as negative </p>\n",
    "<p><b>False Positive</b> - Samples that are actual-negatives incorrectly predicted as positive </p>\n",
    "\n",
    "<p><b>Logistic Loss Function</b> is parabolic in nature. It has an important property of not only telling us the loss at a given weight, but also tells us which way to go to minimize loss</p>\n",
    "<p><b>Gradient Descent</b> optimization alogrithm uses loss function to move the weights of all the features and iteratively adjusts the weights until optimal value is reached</p>\n",
    "\n",
    "<p><b>Batch Gradient Descent</b> predicts y value for all training examples and then adjusts the value of weights based on loss. It can converge much slower when training set is very large. Training set order does not matter as every single example in the training set is considered before making adjustments</p>\n",
    "\n",
    "<p><b>Stochastic Gradient Descent</b> predicts y value for next training example and immediately adjusts the value of weights.</p> It can converge faster when training set is very large.  Training set should be random order otherwise model will not learn correctly.  <b>AWS ML uses Stochastic Gradient Descent</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:ai-ml-dl]",
   "language": "python",
   "name": "conda-env-ai-ml-dl-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
